When does an output require gradient?;;When 1 or more of its inputs require gradient.;

Variable: requires_grad?;;Default False (?) except for pretrained models. Turn off for all variables to disable computing gradients in a subtree of your graph.;

Variable: volatile?;;Similar to require_grad, but only a single volatile var is needed to turn off gradients/intermediate states for all connected operations.;

Check if a model will store intermediate state, compute gradients?;;;a.required_grad<br>a.grad_fn

What data structure does autograd use to build up state during forwards and backwards passes?;;Forward pass: every Variable with a grad_fn is a leaf in a DAG, they're joined until we get to the root which is the output. Backwards pass happens in reverse. Graph built at every iteration, enabling us to change shape of graph dynamically.;

How do you determine if two tensors are broadcastable?;;Align the trailing axes. All dims must be a) the same, b) 1, or c) not exist.;

Initialize a tensor on a GPU?;;;torch.cuda.Tensor()

Set all values in a tensor with 10s?;;;X.fill_(10)

Save/load a tensor to a file?;;;torch.save(X, 'temp.pt')<br>Y = torch.load('temp.pt')

Remove dimension(s) of size 1?;;X<br>dim: only this dim is squeezed<br>out;torch.squeeze(X)

Best way to save/load a model?;;Just save params, or else data is bound to classes used, dir structure, etc. Fragile.;torch.save(mode.state_dict(), PATH)<br>model.load_state_dict(torch.load(PATH))

Add dimension to end of array?;;;torch.unsqueeze(X, -1)

Split a tensor into 5 pieces along a dimension?;;tensor, chunks, dim;torch.chunk(X, 5, 2)

Torch equivalent of boolean indexing?;;;mask = X.gt(5)<br>X.masked_select(mask)

Specify optimization params on a per-layer basis?;;;

Clear optimizer state?;;Clears gradients from all optimized variables.;zero_grad()
