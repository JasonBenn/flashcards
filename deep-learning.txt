Describe the Word2Vec skip gram architecture;;Train simple NN with 1 hidden layer to predict the middle word in a phrase.;

How is an autoencoders useful? They just pass inputs through a hidden layer and output the same thing, right?;;After training, you can use the hidden layer - it'll have learned useful features.;

Autoencoder vs variational autoencoder?;;VAE: loss function includes KL divergence between input encoding and Gaussian uniform vector;

Stochastic gradient descent vs gradient descent?;;Gradient descent: compute gradient for entire set.<br>Stochastic: approximate that gradient by selecting a random batch.;

Cosine distance vs Euclidean distance?;;Cosine = angle between vectors, Euclidean = distance. For high dim space (100s+), distance is meaningless because everything is "on the edge" of that space.;

Find the most similar vector?;;;sklearn.neighbors.NearestNeighbors

sklearn.neighbors: NearestNeighbors vs LSHForest?;;Exact NN (n^2) vs approximate NN (nlog(n)) - locality sensitive hashing forests.;

Convert bcolz array into numpy array?;;Careful you have enough memory tho.;bcolz.open('thing.bc')[:]

Style target for style transfer?;;Gram matrix of image channels. Somehow removes location info, preserves texture info;

Dropout?;;Regularization technique. Analogous to training a large ensemble of small networks.;

Why do word embeddings work by counting context words?;;Because the meaning of "club" in "a ___ walked into a club" depends on if ___ is "DJ" or "baby seal".;

Bilinear interpolation?;;For a point within a field, take the linear average in two dimensions. Compare to nearest-neighbor interpolation.<img src="bilinear-interpolation.png">;

Describe this convolution?<img src="conv-stride.png">;;3x3 filter, stride 2;

How are resnets like ensembles?;;It's in the title: "Residual networks are exponential ensembles of relatively shallow networks", Andreas Veit, May 16;

Momentum vs Nesterov momentum?;;Evaluate gradient at current position, or evalute gradient at end of momentum step? Nesterov consistently a bit better.;

L-BFGS?;;Approximates Newton's method by watching gradients over time to form an approximate Hessian.;

Adagrad, RMSProp?;;Optimization algorithms that maintain different LRs for each param based on magnitudes of their gradients.;

Adam?;;RMSProp with momentum.<br>Often worth trying SGD + Nesterov momentum too, though.;

Why are autoencoders a Bad Idea for unsupervised learning?;;Because they learn to predict the present.<br>Real unsupervised learning happens when you use causal prediction as the supervisory signal (predict next word, next frame) (LSTMs)<br>See: language modeling, autoregressive image generators (PixelRNN) that produce sharper images than non-autoregressive VAEs;

Attention Is All You Need?;;seq2seq. RNNs/CNNs are unparallelizable. Transformer iteratively generates embeddings with multi-headed attention. 2x faster to train, beat SOTA by 2 BLEU;Jun 17, Ashish Vaswani

Artistic style transfer for videos?;;style transfer each frame + a penalty term for diverging from the previous frame + tricks to make it look good:<br>1. penalize deviations along "optical flow"<br>2. don't penalize disoccluded spots<br>3. don't let artifacts at the frame edges form;Apr '16

Pixel RNNs?;;Creates good distributions of images by using 2D RNNs on pixels, with RNN skip connections.; Jan '16

Learning Hierarchical Features for Scene Labeling? Lecun, 2013;;Predict a class for every pixel at different levels of granularity (3-level Laplacian pyramid)<br>Smooth noisy predictions by identifying hierarchies of superpixel regions of similar color intensities, choose labels and regions that minimize surprisingness.<br>;

Laplacian pyramid?;;Multiple scaled copies of an image. Each image is a difference image of one below, lowest-res is pixels. Useful for compression, image segmentation.<br>Compare to Gaussian pyramids: each level is blurred and downsampled.;

Superpixels?;;Image segmentation<br>Hierarchical regions of similar color intensity. Follow natural contours of image.;

Recursive Context Propagation Network?;;An image segmentation architecture w CNN+RNN.<br>RNN resolves confusion between "boat"/"building" using context from other class probabilities, e.g. the nearby segment labeled "water".;

How might you tune your performance metric if one type of mistake is more costly than another?;;Make the cost of certain mistakes higher than others.;

How do you design a performance metric for a rare disease detection system with 99.999% negative examples?;;Instead of measuring error, measure precision (% of predictions that are correct) and recall (% of true events predicted).<br><img src="precision-recall.png">;

If you're considering letting your system not make predictions if its below a certain confidence, what should you measure?;;Coverage - % of things it made a prediction for<br>Accuracy - % of good predictions. OR<br>Precision & recall.;

If your training performance is bad, what should you try?;;Increase model's capacity<br>Tune your LR<br>Improve the quality of your data;

If your test performance is bad, what should you try?;;Reduce the model size<br>Regularize<br>Estimate how much data you need.;

If training error is low and test error is high, how can you brute force your model?;;Increase capacity and add data.;

WGAN?;;Wasserstein GAN. Original GANs used JS divergence for loss, but this isn't defined everywhere.<br>WGANs use Earth Mover distance: continuous, but intractable. So approximate it with another NN.<br>Jan 2017.;








