Describe the Word2Vec skip gram architecture;;Train simple NN with 1 hidden layer to predict the middle word in a phrase.;

How is an autoencoders useful? They just pass inputs through a hidden layer and output the same thing, right?;;After training, you can use the hidden layer - it'll have learned useful features.;

Autoencoder vs variational autoencoder?;;VAE: loss function includes KL divergence between input encoding and Gaussian uniform vector;

Stochastic gradient descent vs gradient descent?;;Gradient descent: compute gradient for entire set.<br>Stochastic: approximate that gradient by selecting a random batch.;

Cosine distance vs Euclidean distance?;;Cosine = angle between vectors, Euclidean = distance. For high dim space (100s+), distance is meaningless because everything is "on the edge" of that space.;

Find the most similar vector?;;;sklearn.neighbors.NearestNeighbors

sklearn.neighbors: NearestNeighbors vs LSHForest?;;Exact NN (n^2) vs approximate NN (nlog(n)) - locality sensitive hashing forests.;

Convert bcolz array into numpy array?;;Careful you have enough memory tho.;bcolz.open('thing.bc')[:]

Style target for style transfer?;;Gram matrix of image channels. Somehow removes location info, preserves texture info;

Dropout?;;Regularization technique. Analogous to training a large ensemble of small networks.;

Why do word embeddings work by counting context words?;;Because the meaning of "club" in "a ___ walked into a club" depends on if ___ is "DJ" or "baby seal".;

Bilinear interpolation?;;For a point within a field, take the linear average in two dimensions. Compare to nearest-neighbor interpolation.<img src="bilinear-interpolation.png">;

Describe this convolution?<img src="conv-stride.png">;;3x3 filter, stride 2;

How are resnets like ensembles?;;It's in the title: "Residual networks are exponential ensembles of relatively shallow networks", Andreas Veit, May 16;

Momentum vs Nesterov momentum?;;Evaluate gradient at current position, or evalute gradient at end of momentum step? Nesterov consistently a bit better.;

L-BFGS?;;Approximates Newton's method by watching gradients over time to form an approximate Hessian.;

Adagrad, RMSProp?;;Optimization algorithms that maintain different LRs for each param based on magnitudes of their gradients.;

Adam? Kingma, Q4 2014;;RMSProp with momentum.<br>Often worth trying SGD + Nesterov momentum too, though.;

Why are autoencoders a Bad Idea for unsupervised learning?;;Because they learn to predict the present.<br>Real unsupervised learning happens when you use causal prediction as the supervisory signal (predict next word, next frame) (LSTMs)<br>See: language modeling, autoregressive image generators (PixelRNN) that produce sharper images than non-autoregressive VAEs;

Attention Is All You Need?;;seq2seq. RNNs/CNNs are unparallelizable. Transformer iteratively generates embeddings with multi-headed attention. 2x faster to train, beat SOTA by 2 BLEU;Jun 17, Ashish Vaswani

Artistic style transfer for videos?;;style transfer each frame + a penalty term for diverging from the previous frame + tricks to make it look good:<br>1. penalize deviations along "optical flow"<br>2. don't penalize disoccluded spots<br>3. don't let artifacts at the frame edges form;Apr '16

Pixel RNNs?;;Creates good distributions of images by using 2D RNNs on pixels, with RNN skip connections.; Jan '16

Learning Hierarchical Features for Scene Labeling? Lecun, 2013;;Predict a class for every pixel at different levels of granularity (3-level Laplacian pyramid)<br>Smooth noisy predictions by identifying hierarchies of superpixel regions of similar color intensities, choose labels and regions that minimize surprisingness.<br>;

Laplacian pyramid?;;Multiple scaled copies of an image. Each image is a difference image of one below, lowest-res is pixels. Useful for compression, image segmentation.<br>Compare to Gaussian pyramids: each level is blurred and downsampled.;

Superpixels?;;Image segmentation<br>Hierarchical regions of similar color intensity. Follow natural contours of image.;

Recursive Context Propagation Network?;;An image segmentation architecture w CNN+RNN.<br>RNN resolves confusion between "boat"/"building" using context from other class probabilities, e.g. the nearby segment labeled "water".;

How might you tune your performance metric if one type of mistake is more costly than another?;;Make the cost of certain mistakes higher than others.;

How do you design a performance metric for a rare disease detection system with 99.999% negative examples?;;Instead of measuring error, measure precision (% of predictions that are correct) and recall (% of true events predicted).<br><img src="precision-recall.png">;

If you're considering letting your system not make predictions if its below a certain confidence, what should you measure?;;Coverage - % of things it made a prediction for<br>Accuracy - % of good predictions. OR<br>Precision & recall.;

If your training performance is bad, what should you try?;;Increase model's capacity<br>Tune your LR<br>Improve the quality of your data;

If your test performance is bad, what should you try?;;Reduce the model size<br>Regularize<br>Estimate how much data you need.;

If training error is low and test error is high, how can you brute force your model?;;Increase capacity and add data.;

WGAN?;;Wasserstein GAN. Original GANs used JS divergence for loss, but this isn't defined everywhere.<br>WGANs use Earth Mover distance: continuous, but intractable. So approximate it with another NN.<br>Jan 2017.;

DCGAN?;;Deep conv GANs - generate realistic images.;

DiscoGAN/CycleGAN?;;Cross-domain relations - handbags to shoes, horses to zebras. Different loss functions.;

Dynamic Routing Between Capsules?;Hinton, Q4 '17;A capsule = a groups of conv filters that output a vector, compared to CNNs which output one value for feature detector. Capsule length represents likelihood of that part existing in input.<br>Margin loss: length of final capsules compared to one-hot encoded input, reconstructive loss: decode vector into image and compare to input (encourages capsules to encode meaningful features)<br>SOTA MNIST, 10.2% on CIFAR10<br>Uses attention-like "routing by agreement" idea to route low-level capsules to high-level capsules whose activations point in similar direction. Replaces pooling, where you lose tons of info;

Polygon RNN?;;Predict polygons from clicks, 5-7x speedup w 80% agreement;

SynthMed?;;Hyperrealistic synthesized medical dataset. Segmented retinal images from DRIVE database.;

Synthetic EHRs: name, pros and cons?;;medGAN. Generally indistinguishable except for outliers (prostate cancer (male) + menopausal disorder in same record - could be weeded out with domain-specific rules). Great for avoiding privacy concerns.;

Training object class detectors with click supervision?;;Weakly supervised object localization: you click middle, it draws bbox. Big speedup;

Cut/Paste/Learn: Synthesis for Instance Detection;;Put real object instances on fake bgs, train. 90% synthetic data outperforms models trained on 100% real data!;

Generate 100% synthetic medical time series data? Pros, performance comparison?;;RGAN (Recurrent GAN), generates time series data w labels (trained early warning system on EHRs from ICUs). Models could train on 100% synthetic data, still generalize well (kinda like ImageNet), no privacy concerns, almost the same performance.;

Knowledge distillation?;;Use an ensemble of models' predictions to train a single model - faster inference, obfuscates training data used.;

Active learning?;;Decide which images to label next;

Online learning?;;Continually train model with new examples;

Weak supervision?;;Use clues.;

Dual Generative Adversarial Networks?;;Generate synthetic segmentation masks, then another GAN converts to images. 1% diff in F1 score, awesome. Sidesteps privacy concerns.;

U-Net?;;Biomedical image segmentation that's fast and requires <30 examples. U-shaped architecture max-pools then deconvs. Use lots of biomedical-specific data aug (warping, fuzzy membranes). Foreground/background class.;
