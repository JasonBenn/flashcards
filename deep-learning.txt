Describe the Word2Vec skip gram architecture;;Train simple NN with 1 hidden layer to predict the middle word in a phrase.;

How is an autoencoders useful? They just pass inputs through a hidden layer and output the same thing, right?;;After training, you can use the hidden layer - it'll have learned useful features.;

Autoencoder vs variational autoencoder?;;VAE: loss function includes KL divergence between input encoding and Gaussian uniform vector;

Stochastic gradient descent vs gradient descent?;;Gradient descent: compute gradient for entire set.<br>Stochastic: approximate that gradient by selecting a random batch.;

Cosine distance vs Euclidean distance?;;Cosine = angle between vectors, Euclidean = distance. For high dim space (100s+), distance is meaningless because everything is "on the edge" of that space.;

Find the most similar vector?;;;sklearn.neighbors.NearestNeighbors

sklearn.neighbors: NearestNeighbors vs LSHForest?;;Exact NN (n^2) vs approximate NN (nlog(n)) - locality sensitive hashing forests.;

Convert bcolz array into numpy array?;;Careful you have enough memory tho.;bcolz.open('thing.bc')[:]

Style target for style transfer?;;Gram matrix of image channels. Somehow removes location info, preserves texture info;

Dropout?;;Regularization technique. Analogous to training a large ensemble of small networks.;

Why do word embeddings work by counting context words?;;Because the meaning of "club" in "a ___ walked into a club" depends on if ___ is "DJ" or "baby seal".;

Attention Is All You Need?;;seq2seq. RNNs/CNNs are unparallelizable. Transformer iteratively generates embeddings with multi-headed attention. 2x faster to train, beat SOTA by 2 BLEU;Jun 17, Ashish Vaswani

Bilinear interpolation?;;For a point within a field, take the linear average in two dimensions. Compare to nearest-neighbor interpolation.<img src="bilinear-interpolation.png">;

Describe this convolution?<img src="conv-stride.png">;;3x3 filter, stride 2;

How are resnets like ensembles?;;It's in the title: "Residual networks are exponential ensembles of relatively shallow networks", Andreas Veit, May 16;

Why are autoencoders a Bad Idea for unsupervised learning?;;Because they learn to predict the present.<br>Real unsupervised learning happens when you use causal prediction as the supervisory signal (predict next word, next frame) (LSTMs)<br>See: language modeling, autoregressive image generators (PixelRNN) that produce sharper images than non-autoregressive VAEs;
