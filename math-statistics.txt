How can you recognize overfitting?;;Training MSE is small, testing MSE is large.<br>Reduce the flexibility of your model.;

Softmax?;;Translate numbers into "probabilities". Invariant to bias.;e^X/e^X.sum()

When is the span of two vectors *not* any point on their plane?;;When aligned: you'd only be able to reach points on a single line. Also, when they're both zero.;

Linear dependence?;;When you can remove at least one vector without affecting the span.;

If a matrix's determinant is zero, what does that mean?;;The transformation collapsed along at least one dimension, so the volume is now 0. Columns (transf. vectors) must be linearly dependent.;

Determinant?;;How much areas change due to the transformation defined by the matrix's vectors (so it has to be square). Imagine a parallelepiped getting stretched out or compressed (or inverted).;

Dot product?;;Project one vector onto another, multiply lengths. Dot product of 0 = vectors are orthogonal. Helpful to understand a vector as a transformation.;

np: vdot vs dot?;;dot, when passed matrices, will do a matrix product.<br>vdot will flatten matrices into vectors and then do the dot product.<br>therefore: vdot should only be used for vectors.;

np: Euclidean norm of a matrix?;;AKA the Frobenius norm<br>ord: order of the norm or 'fro'<br>axis: int or 2-tuple of int<br>keepdims=False: keep axes that were normed so broadcast works (?);np.linalg.norm(x, 'fro')

Matrix rank?;;Dimensionality of vector space spanned by matrix's column vectors AKA # of linearly independent vectors of matrix.;

np: sum diagonal of matrix?;;;np.trace(x)

Inverse of a matrix?;;When you multiply it by your matrix, it results in an identity matrix.;x @ y == np.identity(n)

Perplexity?;;How well a prob distribution predicts a sample. Low perplexity means is good/unsurprising.;

Log loss?;;Classification loss fn that penalizes overconfidence. AKA logistic loss, cross-entropy loss.;

Span?;;By scaling your vectors, how many points in a space can you touch? All of them, or less dimensions than that?;

What are norms?;;Ways to measure the size of vectors (length from origin);

Norm formula?;;L1: Σ|x|<br>Ln: Σ(|x|^p)^1/p;

When should you use an L1 over an L2?;;;

Why is it convenient and fast to use the square L2 norm?;;Squared L2 can be computed with X.T @ X<br>Derivatives with respect to each x only require x, whereas L2 norm derivs require whole vector.;

Frobenius norm?;;For taking size of matrices. Analogous to vector L2 norm: you square everything, sum it, then root it.;

Compute cross-entropy/log loss?;;-Σ yᵢ log(ŷᵢ);

Compute the derivate of f at a?;;<img src="derivative.svg">;

How should you imagine an inverse matrix?;;A matrix is a linear transformation into a weird coordinate system: its inverse is playing that backwards (so you're left with the identity: a set of basis vectors!);

How should I imagine matrix multiplication?;;Successive linear transformations.<br>2x2 @ 2x2 results in a 2x2!;

How does Grant Sanderson describe ops like A⁻¹MA?;;As an "empathetic" transformation M (in her language). Start w Jenni's vector _v: A transforms to our language, M is a transform in our language, and A⁻¹ reverts to her language.<img src="translating-a-transform.png">;

How to think about a diagonal matrix?;;All cols are eigenvectors, and those values are eigenvalues!;

How to think of eigenvectors?;;The vectors that stay on their own span during a transformation.;

Singular matrix?;;Square, linearly dependent columns (det = 0). Singular because it's the only kind of matrix without an inverse!;

Eigenvector _x and eigenvalue x?;;X @ _x only alters scale of eigenvec (by eigenvalue)<br>Helpful because they tell us how X scales space in each dimension.;

Eigendecomposition?;;Vdiag(λ)V⁻¹<br>V: eigenvectors, λ: diagonal of eigenvalues;

Useful properties of eigendecomposition?;;If any eigenvalues are 0, matrix is singular.<br>Useful for PCA - you can rank eigenvectors by eigenvalues, throw away the rest.;

Linear transformation vs transformation?;;Origin stays put, and gridlines stay parallel and evenly spaced. Other transformations are too complex for linalg!;

Why do we say that matrix columns represent vectors?;;So that matrices represent transforms, and X @ x multiplication represents result of linear transformation on x.<br>Once you start thinking of square matrices as transformations of space, so many concepts get easier: matmul, determinants, eigenv's.;

Positive definite vs positive semidefinite vs negative definite vs negative semidefinite?;;Describes eigenvalues: whether they're all positive/negative and whether they include zeros.;

How do we find the inverse of a matrix that isn't square?;;Moore-Penrose Pseudoinverse, powered by rearranging results of SVD.;

Singular value decomposition?;;UDVᵀ<br>A decomp that exists for all real matrices, helpful for computing inverses.<br>U: left-singular vectors<br>D: singular values across diagonal<br>V: right-singular vectors;

Complex conjugate?;;Complex number, but flip the sign of the imaginary bit. Mirror image/evil twin;

Hermitian matrix and conjugate transpose?;;Like a symmetric matrix for complex numbers.<br>CT: transpose a matrix, take complex conjugates of all values.<br>Hermitian: equal to its own CT.<br>Always have real eigenvalues (same as symmetric m's).;

Cholesky decomposition?;;Only for hermitian, positive definite m's.<br>;

When do matrices have eigendecompositions?;;When they're diagonalizable (similar to a diagonal matrix);

Jacobian?;;For functions whose m inputs and n outputs are vectors: a matrix nxm of partial derivatives.;

Hessian?;;A matrix of second derivatives/a Jacobian of the gradient.;

Condition number?;;If max singular value is big multiple of min singular value (eigenvalues for normal matrices), the matrix is "poorly conditioned". Poorly conditioned Hessians are bad news for gradient descent.;

What can the eigendecomposition of a Hessian tell us?;;The directions and scale of the second derivatives of a function.<br>Positive/negative definite: we're at local minima/maxima.<br>Mixed sign eigenvalues: saddle points.<br>Any zeros: inconclusive.;If our fn has a poorly conditioned Hessian, that tells us that one dimension is changing quickly, and that we should chase in that direction instead of following the steepest first derivative. Imagine standing high on a canyon walls going down a mountain: 1st order with too high a learning rate could zig-zag you across the canyon, 2nd order takes you down the mountain directly.

Newton's method?;;Second-order optimization technique. Uses info from Hessian to guide descent. Can jump right to solution if problem is quadratic. If only approximating quadratic, iteratively use it to jump to solution much faster than grad descent. Attracts to saddle points, unfortunately;

First-order optimization vs second-order optimization?;;Does the algo use just the gradient, or does it also use the Hessian?;

Convex optimization?;;Function for which the Hessian is positive semidefinite everywhere. Great for second-order optimization.;

MLE?;;Maximum likelihood estimation.<br>Estimate the distribution given a sample (i.e., estimate μ, σ² for all dims)<br>;

Calculus: product rule? fg;;;f'g + fg'

Calculus: quotient rule? f/g;;;(f'g - fg')/g²

Calculus: reciprocal rule? 1/f;;;-f'/f²









