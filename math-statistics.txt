How can you recognize overfitting?;;Training MSE is small, testing MSE is large.<br>Reduce the flexibility of your model.;

Softmax?;;Translate numbers into "probabilities". Invariant to bias.;e^X/e^X.sum()

When is the span of two vectors *not* any point on their plane?;;When aligned: you'd only be able to reach points on a single line. Also, when they're both zero.;

Linear dependence?;;When you can remove at least one vector without affecting the span.;

If a matrix's determinant is zero, what does that mean?;;The transformation collapsed along at least one dimension, so the volume is now 0. Columns (transf. vectors) must be linearly dependent.;

Determinant?;;How much areas change due to the transformation defined by the matrix's vectors (so it has to be square). Imagine a parallelepiped getting stretched out or compressed (or inverted).;

Dot product?;;Project one vector onto another, multiply lengths. Dot product of 0 = vectors are orthogonal. Helpful to understand a vector as a transformation.;

np: vdot vs dot?;;dot, when passed matrices, will do a matrix product.<br>vdot will flatten matrices into vectors and then do the dot product.<br>therefore: vdot should only be used for vectors.;

np: Euclidean norm of a matrix?;;AKA the Frobenius norm<br>ord: order of the norm or 'fro'<br>axis: int or 2-tuple of int<br>keepdims=False: keep axes that were normed so broadcast works (?);np.linalg.norm(x, 'fro')

Matrix rank?;;Dimensionality of vector space spanned by matrix's column vectors AKA # of linearly independent vectors of matrix.;

np: sum diagonal of matrix?;;;np.trace(x)

Inverse of a matrix?;;When you multiply it by your matrix, it results in an identity matrix.;x @ y == np.identity(n)

Perplexity?;;How well a prob distribution predicts a sample. Low perplexity means is good/unsurprising.;

Log loss?;;Classification loss fn that penalizes overconfidence. AKA logistic loss, cross-entropy loss.;

Span?;;By scaling your vectors, how many points in a space can you touch? All of them, or less dimensions than that?;

What are norms?;;Ways to measure the size of vectors (length from origin);

Norm formula?;;L1: Σ|x|<br>Ln: Σ(|x|^p)^1/p;

When should you use an L1 over an L2?;;;

Why is it convenient and fast to use the square L2 norm?;;Squared L2 can be computed with X.T @ X<br>Derivatives with respect to each x only require x, whereas L2 norm derivs require whole vector.;

Frobenius norm?;;For taking size of matrices. Analogous to vector L2 norm: you square everything, sum it, then root it.;

Compute cross-entropy/log loss?;;-Σ yᵢ log(ŷᵢ);

Compute the derivate of f at a?;;<img src="derivative.svg">;

How should you imagine an inverse matrix?;;A matrix is a linear transformation into a weird coordinate system: its inverse is playing that backwards (so you're left with the identity: a set of basis vectors!);

How should I imagine matrix multiplication?;;Successive linear transformations.<br>2x2 @ 2x2 results in a 2x2!;

How does Grant Sanderson describe ops like A⁻¹MA?;;As an "empathetic" transformation M (in her language). Start w Jenni's vector _v: A transforms to our language, M is a transform in our language, and A⁻¹ reverts to her language.<img src="translating-a-transform.png">;

How to think about a diagonal matrix?;;All cols are eigenvectors, and those values are eigenvalues!;

How to think of eigenvectors?;;The vectors that stay on their own span during a transformation.;

Singular matrix?;;Square, linearly dependent columns (det = 0). Singular because it's the only kind of matrix without an inverse!;

Eigenvector _x and eigenvalue x?;;X @ _x only alters scale of eigenvec (by eigenvalue)<br>Helpful because they tell us how X scales space in each dimension.;

Eigendecomposition?;;Vdiag(λ)V⁻¹<br>V: eigenvectors, λ: diagonal of eigenvalues;

Useful properties of eigendecomposition?;;If any eigenvalues are 0, matrix is singular.<br>Useful for PCA - you can rank eigenvectors by eigenvalues, throw away the rest.;

Linear transformation vs transformation?;;Origin stays put, and gridlines stay parallel and evenly spaced. Other transformations are too complex for linalg!;

Why do we say that matrix columns represent vectors?;;So that matrices represent transforms, and X @ x multiplication represents result of linear transformation on x.<br>Once you start thinking of square matrices as transformations of space, so many concepts get easier: matmul, determinants, eigenv's.;

Positive definite vs positive semidefinite vs negative definite vs negative semidefinite?;;Describes eigenvalues: whether they're all positive/negative and whether they include zeros.;

How do we find the inverse of a matrix that isn't square?;;Moore-Penrose Pseudoinverse, powered by rearranging results of SVD.;

Singular value decomposition?;;UDVᵀ<br>A decomp that exists for all real matrices, helpful for computing inverses.<br>U: left-singular vectors<br>D: singular values across diagonal<br>V: right-singular vectors;

Complex conjugate?;;Complex number, but flip the sign of the imaginary bit. Mirror image/evil twin;

Hermitian matrix and conjugate transpose?;;Like a symmetric matrix for complex numbers.<br>CT: transpose a matrix, take complex conjugates of all values.<br>Hermitian: equal to its own CT.<br>Always have real eigenvalues (same as symmetric m's).;

Cholesky decomposition?;;Only for hermitian, positive definite m's.<br>;

