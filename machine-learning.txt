Active learning?;;Decide which images to label next;
Adagrad, RMSProp?;;Optimization algorithms that maintain different LRs for each param based on magnitudes of their gradients.;
Adam? Kingma, Q4 2014;;RMSProp with momentum.<br>Often worth trying SGD + Nesterov momentum too, though.;
Add dimension to end of array?;;;torch.unsqueeze(X, -1)
Artistic style transfer for videos?;;style transfer each frame + a penalty term for diverging from the previous frame + tricks to make it look good:<br>1. penalize deviations along "optical flow"<br>2. don't penalize disoccluded spots<br>3. don't let artifacts at the frame edges form;Apr '16
Attention Is All You Need?;;seq2seq. RNNs/CNNs are unparallelizable. Transformer iteratively generates embeddings with multi-headed attention. 2x faster to train, beat SOTA by 2 BLEU;Jun 17, Ashish Vaswani
Autoencoder vs variational autoencoder?;;VAE: loss function includes KL divergence between input encoding and Gaussian uniform vector;
What's the old-school, dumb question answering dataset?;;bAbI: AI-Complete Question Answering. 10k, synthetic. Each task is a story, query, answer. Stories are ordered in time and you have to get the answer at the end. Dumb, but used for memory networks.;
Best way to save/load a model?;;Just save params, or else data is bound to classes used, dir structure, etc. Fragile.;torch.save(mode.state_dict(), PATH)<br>model.load_state_dict(torch.load(PATH))
Bilinear interpolation?;;For a point within a field, take the linear average in two dimensions. Compare to nearest-neighbor interpolation.<img src="bilinear-interpolation.png">;
Calculus: product rule? fg;;;f'g + fg'
Calculus: quotient rule? f/g;;;(f'g - fg')/g²
Calculus: reciprocal rule? 1/f;;;-f'/f²
Check if a model will store intermediate state, compute gradients?;;;a.required_grad<br>a.grad_fn
Cholesky decomposition?;;Only for hermitian, positive definite m's.<br>;
Clear optimizer state?;;Clears gradients from all optimized variables.;zero_grad()
Complex conjugate?;;Complex number, but flip the sign of the imaginary bit. Mirror image/evil twin;
Compute cross-entropy/log loss?;;-Σ yᵢ log(ŷᵢ)<br>Sum of the labels times log of predictions.;
Condition number?;;If max singular value is big multiple of min singular value (eigenvalues for normal matrices), the matrix is "poorly conditioned". Poorly conditioned Hessians are bad news for gradient descent.;
Convert bcolz array into numpy array?;;Careful you have enough memory tho.;bcolz.open('thing.bc')[:]
Convex optimization?;;Function for which the Hessian is positive semidefinite everywhere. Great for second-order optimization.;
Cosine distance vs Euclidean distance?;;Cosine = angle between vectors, Euclidean = distance. For high dim space (100s+), distance is meaningless because everything is "on the edge" of that space.;
Count rows in a DataFrame that have any value that satisfies some predicate?;;;(train_X == 0).any(axis=0)
Count values in a Series that satisfy some predicate?;;;sum(train_y > 2)
Cut/Paste/Learn: Synthesis for Instance Detection;;Put real object instances on fake bgs, train. 90% synthetic data outperforms models trained on 100% real data!;
Data helps, but subject to diminishing returns: e.g., diabetic retinopathy prediction accuracy maxed out around 50k - not intractable for a startup.;;;
DCGAN?;;Deep conv GANs - generate realistic images.;
Deep RL models also haven't had their batchnorm moment - they don't yet "want" to train. You have to fight them one hyperparam at a time.;;;
Describe the Word2Vec skip gram architecture;;Train simple NN with 1 hidden layer to predict the middle word in a phrase.;
Describe this convolution?<img src="conv-stride.png">;;3x3 filter, stride 2;
Determinant?;;How much areas change due to the transformation defined by the matrix's vectors (so it has to be square). Imagine a parallelepiped getting stretched out or compressed (or inverted).;
Diabetic retinopathy?;;Diabetes leading to blindness. Vessels all fucked up. Hemorrhages/aneurysms, hard exudates, abnormal growth.;
Differential diagnosis?;;Bayesian diagnosing: updating probabilities of multiple possible diagnoses based on evidence;
DiscoGAN/CycleGAN?;;Cross-domain relations - handbags to shoes, horses to zebras. Different loss functions.;
DiscoGAN?;;Cross-domain relations - handbags to shoes, horses to zebras;
Dot product?;;Project one vector onto another, multiply lengths. Dot product of 0 = vectors are orthogonal. Helpful to understand a vector as a transformation.;
Dropout?;;Regularization technique. Analogous to training a large ensemble of small networks.;
Dual Generative Adversarial Networks?;;Generate synthetic segmentation masks, then another GAN converts to images. 1% diff in F1 score, awesome. Sidesteps privacy concerns.;
Dynamic Routing Between Capsules?;Hinton, Q4 '17;A capsule = a groups of conv filters that output a vector, compared to CNNs which output one value for feature detector. Capsule length represents likelihood of that part existing in input.<br>Margin loss: length of final capsules compared to one-hot encoded input, reconstructive loss: decode vector into image and compare to input (encourages capsules to encode meaningful features)<br>SOTA MNIST, 10.2% on CIFAR10<br>Uses attention-like "routing by agreement" idea to route low-level capsules to high-level capsules whose activations point in similar direction. Replaces pooling, where you lose tons of info;
Eigendecomposition?;;Vdiag(λ)V⁻¹<br>V: eigenvectors, λ: diagonal of eigenvalues;
Feature matching?;;Stitching images together by identifying aspects of objects in the image that are less variant when you view them from different angles. Combination of feature maps = feature descriptor.;
Few-shot learning?;;Learn a good weight init so learning converges rapidly<br>Learn embeddings of examples that classify w linear model better.;
Find the most similar vector?;;;sklearn.neighbors.NearestNeighbors
First-order optimization vs second-order optimization?;;Does the algo use just the gradient, or does it also use the Hessian?;
Frobenius norm?;;For taking size of matrices. Analogous to vector L2 norm: you square everything, sum it, then root it.;
GANs, one of the more exciting developments, has yet to have its "batch norm" moment, where everything wants to train. You have to fight them one hyperparam at a time.;;;
GBMs vs RFs?;;GBMs: start with high bias, low variance tree. Reduce by training additional trees on residuals.<br>RFs: low bias, high variance. Reduce variance by ensembling.;
Generate 100% synthetic medical time series data? Pros, performance comparison?;;RGAN (Recurrent GAN), generates time series data w labels (trained early warning system on EHRs from ICUs). Models could train on 100% synthetic data, still generalize well (kinda like ImageNet), no privacy concerns, almost the same performance.;
Hermitian matrix and conjugate transpose?;;Like a symmetric matrix for complex numbers.<br>CT: transpose a matrix, take complex conjugates of all values.<br>Hermitian: equal to its own CT.<br>Always have real eigenvalues (same as symmetric m's).;
Hessian?;;A matrix of second derivatives/a Jacobian of the gradient.;
How are resnets like ensembles?;;It's in the title: "Residual networks are exponential ensembles of relatively shallow networks", Andreas Veit, May 16;
How can you recognize overfitting?;;Training MSE is small, testing MSE is large.<br>Reduce the flexibility of your model.;
How did Lecun smooth high-variance per-pixel predictions in Learning Hierarchical Features for Scene Labeling? (2013);;Well first he predicted a class for every pixel at 3 different levels of granularity (3-level Laplacian pyramid)<br>Then he smoothed noisy predictions by identifying hierarchies of superpixel regions of similar color intensities. Finally he chose labels and regions that minimize surprisingness w a tree structure.<br>;
How did SynthMed generate such a realistic retinal image dataset?;;They used real segmented retinal images and generated the contents;
How do Recursive Context Propagation Networks resolve confusing image segmentation problems, like whether something is a "boat" or a "building"?;;They used an RNN to add context from other class probabilities, e.g. by noticing that the nearby segment is labeled "water".;
How do we find the inverse of a matrix that isn't square?;;Moore-Penrose Pseudoinverse, powered by rearranging results of SVD.;
How do you design a performance metric for a rare disease detection system with 99.999% negative examples?;;Instead of measuring error, measure precision (% of predictions that are correct) and recall (% of true events predicted).<br><img src="precision-recall.png">;
How do you determine if two tensors are broadcastable?;;Align the trailing axes. All dims must be a) the same, b) 1, or c) not exist.;
How does a Pixel RNN (Jan '16) create a decent distribution of images?;;By using 2D RNNs to scan across pixels, and adding RNN skip connections.;
How does Grant Sanderson describe ops like A⁻¹MA?;;As an "empathetic" transformation M (in her language). Start w Jenni's vector _v: A transforms to our language, M is a transform in our language, and A⁻¹ reverts to her language.<img src="translating-a-transform.png">;
How does nn.Embedding work?;;Pass it a minibatch of examples (lists of indexes), they'll be mapped to their embeddings.<br>num_embeddings<br>embedding_dim;e = nn.Embedding(50000, 300)<br>minibatch = Variable(LongTensor([[0, 3, 1]]))<br>e(minibatch)
How is an autoencoders useful? They just pass inputs through a hidden layer and output the same thing, right?;;After training, you can use the hidden layer - it'll have learned useful features.;
How might you tune your performance metric if one type of mistake is more costly than another?;;Make the cost of certain mistakes higher than others.;
How much does biomedical imaging cost?;;1-5 per X-ray, 25-30 per chest CT;
How much does MTurk/Squadrun cost?;;1c/image, 2c/bbox. MTurk more if you want to average responses.;
How should I imagine matrix multiplication?;;Successive linear transformations.<br>2x2 @ 2x2 results in a 2x2!;
How should I think about an eigenvector's (_x) and eigenvalue's (x) effect on a matrix?;;X @ _x alters the scale of the eigenvector (by the eigenvalue)<br>Helpful because they tell us how X scales space in each dimension.;
How should I use torch.optim?;;Instantiate an optimizer object to hold and update params based on gradients.<br>Takes a list or params or a list of dicts of params<br>Each optimizer algorithm takes different kwargs.;
How should you imagine an inverse matrix?;;A matrix is a linear transformation into a weird coordinate system: its inverse is playing that backwards (so you're left with the identity: a set of basis vectors!);
How to think about a diagonal matrix?;;All cols are eigenvectors, and those values are eigenvalues!;
How to think of eigenvectors?;;The vectors that stay on their own span during a transformation.;
I don’t think we’ve really broken through on unsupervised learning. There’s a huge amount of information and structure in the unconditioned data distribution, and it seems like there should be some way for a learning algorithm to benefit from that.;;;
If a matrix's determinant is zero, what does that mean?;;The transformation collapsed along at least one dimension, so the volume is now 0. Columns (transf. vectors) must be linearly dependent.;
If training error is low and test error is high, how can you brute force your model?;;Increase capacity and add data.;
If you're considering letting your system not make predictions if its below a certain confidence, what should you measure?;;Coverage - % of things it made a prediction for<br>Accuracy - % of good predictions. OR<br>Precision & recall.;
If your test performance is bad, what should you try?;;Reduce the model size<br>Regularize<br>Estimate how much data you need.;
If your training performance is bad, what should you try?;;Increase model's capacity<br>Tune your LR<br>Improve the quality of your data;
Initialize a tensor on a GPU?;;;torch.cuda.Tensor()
Inverse of a matrix?;;When you multiply it by your matrix, it results in an identity matrix.;x @ y == np.identity(n)
Jacobian?;;For functions whose m inputs and n outputs are vectors: a matrix nxm of partial derivatives.;
Knowledge distillation?;;Use an ensemble of models' predictions to train a single model - faster inference, obfuscates training data used.;
L-BFGS?;;Approximates Newton's method by watching gradients over time to form an approximate Hessian.;
Laplacian pyramid?;;Multiple scaled copies of an image. Each image is a difference image of one below, lowest-res is pixels. Useful for compression, image segmentation.<br>Compare to Gaussian pyramids: each level is blurred and downsampled.;
Linear dependence?;;When you can remove at least one vector without affecting the span.;
Linear transformation vs transformation?;;Origin stays put, and gridlines stay parallel and evenly spaced. Other transformations are too complex for linalg!;
Making deep networks amenable to (stable) online updates from weakly supervised data is huge. Solving it enables true lifelong learning and opens many applications.;;;
medGAN?;;Synthetic EHRs. Generally indistinguishable except for outliers (prostate cancer (male) + menopausal disorder in same record - could be weeded out with domain-specific rules). Great for avoiding privacy concerns.;
MLE?;;Maximum likelihood estimation.<br>Estimate the distribution given a sample (i.e., estimate μ, σ² for all dims)<br>;
Momentum vs Nesterov momentum?;;Evaluate gradient at current position, or evalute gradient at end of momentum step? Nesterov consistently a bit better.;
Newton's method?;;Second-order optimization technique. Uses info from Hessian to guide descent. Can jump right to solution if problem is quadratic. If only approximating quadratic, iteratively use it to jump to solution much faster than grad descent. Attracts to saddle points, unfortunately;
nn.Linear?;;Linear transformation factory that learns weights and bias for you.;lin = Linear(36, 300, bias=True)<br>relu(lin(inputs))
Nonstandard approach: feedback! "It’s insane to me that we’ve gotten this far with pure feedforward approaches. Dynamical systems are very efficient, adaptive learning machines." RNNs, for example, aren't "loopy" and they propagate information only in one direction: if there is any feedback, it comes from outside the learner. Contrast, e.g. with Markov nets, where information is propagated in both directions within the model.;;;
Norm formula?;;L1: Σ|x|<br>Ln: Σ(|x|^p)^1/p;
np: Euclidean norm of a matrix?;;AKA the Frobenius norm<br>ord: order of the norm or 'fro'<br>axis: int or 2-tuple of int<br>keepdims=False: keep axes that were normed so broadcast works (?);np.linalg.norm(x, 'fro')
np: sum diagonal of matrix?;;;np.trace(x)
np: vdot vs dot?;;dot, when passed matrices, will do a matrix product.<br>vdot will flatten matrices into vectors and then do the dot product.<br>therefore: vdot should only be used for vectors.;
One-short learning?;;Use prior knowledge about categories (share features or parameters) to classify new objects with one example.;
Online learning?;;Continually train model with new examples;
Perplexity?;;How well a prob distribution predicts a sample. Low perplexity means is good/unsurprising.;
Polygon RNN?;;Predict polygons from clicks, 5-7x speedup w 80% agreement;
Positive definite vs positive semidefinite vs negative definite vs negative semidefinite?;;Describes eigenvalues: whether they're all positive/negative and whether they include zeros.;
Pseudolabeling?;;Use predicted classes as if they were true labels;
How big is the Push dataset, of robots pushing seen and unseen objects?;;59k. From Unsupervised Learning for Physical Interaction through Video Prediction. Good for predicting causal future.;
Remove dimension(s) of size 1?;;X<br>dim: only this dim is squeezed<br>out;torch.squeeze(X)
Reshape a PyTorch tensor?;;-1 can be inferred from other dimensions.;tensor.view((-1, 1))
RGAN?;;Recurrent GAN, generates time series data w labels (trained early warning system on EHRs from ICUs). Models could train on 100% synthetic data, still generalize well (kinda like ImageNet), no privacy concerns, almost the same performance.;
Save/load a tensor to a file?;;;torch.save(X, 'temp.pt')<br>Y = torch.load('temp.pt')
Set all values in a tensor with 10s?;;;X.fill_(10)
Shoehorning an unsupervised problem into a large supervised one is half the magic. Then it's data science, model architecture, and good luck.  Some problems can't ever really be captured as supervised, tho - in those cases, DL probably isn't the answer.;;;
Singular matrix?;;Square, linearly dependent columns (det = 0). Singular because it's the only kind of matrix without an inverse!;
Singular value decomposition?;;UDVᵀ<br>A decomp that exists for all real matrices, helpful for computing inverses.<br>U: left-singular vectors<br>D: singular values across diagonal<br>V: right-singular vectors;
sklearn.neighbors: NearestNeighbors vs LSHForest?;;Exact NN (n^2) vs approximate NN (nlog(n)) - locality sensitive hashing forests.;
Softmax?;;Translate numbers into "probabilities". Invariant to bias.;e^X/e^X.sum()
Span?;;By scaling your vectors, how many points in a space can you touch? All of them, or less dimensions than that?;
Specify optimization params on a per-layer basis?;;Pass a list of dicts to optimizer factory: 'params' is list of params, other kwargs are individual settings, like lr.;
Split a tensor into 5 pieces along a dimension?;;tensor, chunks, dim;torch.chunk(X, 5, 2)
Stochastic gradient descent vs gradient descent?;;Gradient descent: compute gradient for entire set.<br>Stochastic: approximate that gradient by selecting a random batch.;
Style target for style transfer?;;Gram matrix of image channels. Somehow removes location info, preserves texture info;
Superpixels?;;Image segmentation<br>Hierarchical regions of similar color intensity. Follow natural contours of image.;
Synthetic EHRs: name, pros and cons?;;medGAN. Generally indistinguishable except for outliers (prostate cancer (male) + menopausal disorder in same record - could be weeded out with domain-specific rules). Great for avoiding privacy concerns.;
The difference between unsupervised learning that works (e.g. language models) and unsupervised learning that doesn’t is generally about predicting the causal future (next word, next frame) instead of the present (autoencoding).;;;
What are some problems with training in the real world, according to Yann Lecun?;;(1) you can't run it faster than real time, (2) you can't back-propagate gradients through it, (3) any action you take can hurt or kill you.;
Torch equivalent of boolean indexing?;;;mask = X.gt(5)<br>X.masked_select(mask)
Training object class detectors with click supervision?;;Weakly supervised object localization: you click middle, it draws bbox. Big speedup;
U-Net?;;Biomedical image segmentation that's fast and requires <30 examples. U-shaped architecture max-pools then deconvs. Use lots of biomedical-specific data aug (warping, fuzzy membranes). Foreground/background class.;
Useful properties of eigendecomposition?;;If any eigenvalues are 0, matrix is singular.<br>Useful for PCA - you can rank eigenvectors by eigenvalues, throw away the rest.;
Variable: requires_grad?;;Default False (?) except for pretrained models. Turn off for all variables to disable computing gradients in a subtree of your graph.;
Variable: volatile?;;Similar to require_grad, but only a single volatile var is needed to turn off gradients/intermediate states for all connected operations.;
Weak supervision?;;Use clues.;
WGAN?;;Wasserstein GAN. Original GANs used JS divergence for loss, but this isn't defined everywhere.<br>WGANs use Earth Mover distance: continuous, but intractable. So approximate it with another NN.<br>Jan 2017.;
What are norms?;;Ways to measure the size of vectors (length from origin);
What can the eigendecomposition of a Hessian tell us?;;The directions and scale of the second derivatives of a function.<br>Positive/negative definite: we're at local minima/maxima.<br>Mixed sign eigenvalues: saddle points.<br>Any zeros: inconclusive.;If our fn has a poorly conditioned Hessian, that tells us that one dimension is changing quickly, and that we should chase in that direction instead of following the steepest first derivative. Imagine standing high on a canyon walls going down a mountain: 1st order with too high a learning rate could zig-zag you across the canyon, 2nd order takes you down the mountain directly.
What data structure does autograd use to build up state during forwards and backwards passes?;;Forward pass: every Variable with a grad_fn is a leaf in a DAG, they're joined until we get to the root which is the output. Backwards pass happens in reverse. Graph built at every iteration, enabling us to change shape of graph dynamically.;
What is a classification loss function that penalizes overconfidence?;;Log loss, AKA logistic loss or cross-entropy loss.;
When do matrices have eigendecompositions?;;When they're diagonalizable (similar to a diagonal matrix);
When does an output require gradient?;;When 1 or more of its inputs require gradient.;
When is the span of two vectors *not* any point on their plane?;;When aligned: you'd only be able to reach points on a single line. Also, when they're both zero.;
When should you use an L1 over an L2?;;;
Why are autoencoders a Bad Idea for unsupervised learning?;;Because they learn to predict the present.<br>Real unsupervised learning happens when you use causal prediction as the supervisory signal (predict next word, next frame) (LSTMs)<br>See: language modeling, autoregressive image generators (PixelRNN) that produce sharper images than non-autoregressive VAEs;
Why are sparse matrices good for performance?;;Can skip multiply/add by zero<br>Way smaller;
Why do we say that matrix columns represent vectors?;;So that matrices represent transforms, and X @ x multiplication represents result of linear transformation on x.<br>Once you start thinking of square matrices as transformations of space, so many concepts get easier: matmul, determinants, eigenv's.;
Why do word embeddings work by counting context words?;;Because the meaning of "club" in "a ___ walked into a club" depends on if ___ is "DJ" or "baby seal".;
Why is it convenient and fast to use the square L2 norm?;;Squared L2 can be computed with X.T @ X<br>Derivatives with respect to each x only require x, whereas L2 norm derivs require whole vector.;
Why is it cool to generate synthetic segmentation masks and then convert them to images?;;In healthcare settings, training on this synthetic data can sidestep privacy concerns. Performance is on par: 1% difference in F1 score. Dual Generative Adversarial Networks.;
Why is it cool to know the "rank" of a matrix?;;Because that's how many dimensions the matrix's column vectors span - AKA, the # of linearly independent vectors of the matrix.;
