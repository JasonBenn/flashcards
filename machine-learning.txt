How can you recognize overfitting?;;Training MSE is small, testing MSE is large.<br>Reduce the flexibility of your model.;

Softmax?;;Translate numbers into "probabilities". Invariant to bias.;e^X/e^X.sum()

When is the span of two vectors *not* any point on their plane?;;When aligned: you'd only be able to reach points on a single line. Also, when they're both zero.;

Linear dependence?;;When you can remove at least one vector without affecting the span.;

If a matrix's determinant is zero, what does that mean?;;The transformation collapsed along at least one dimension, so the volume is now 0. Columns (transf. vectors) must be linearly dependent.;

Determinant?;;How much areas change due to the transformation defined by the matrix's vectors (so it has to be square). Imagine a parallelepiped getting stretched out or compressed (or inverted).;

Dot product?;;Project one vector onto another, multiply lengths. Dot product of 0 = vectors are orthogonal. Helpful to understand a vector as a transformation.;

np: vdot vs dot?;;dot, when passed matrices, will do a matrix product.<br>vdot will flatten matrices into vectors and then do the dot product.<br>therefore: vdot should only be used for vectors.;

np: Euclidean norm of a matrix?;;AKA the Frobenius norm<br>ord: order of the norm or 'fro'<br>axis: int or 2-tuple of int<br>keepdims=False: keep axes that were normed so broadcast works (?);np.linalg.norm(x, 'fro')

Why is it cool to know the "rank" of a matrix?;;Because that's how many dimensions the matrix's column vectors span - AKA, the # of linearly independent vectors of the matrix.;

np: sum diagonal of matrix?;;;np.trace(x)

Inverse of a matrix?;;When you multiply it by your matrix, it results in an identity matrix.;x @ y == np.identity(n)

Perplexity?;;How well a prob distribution predicts a sample. Low perplexity means is good/unsurprising.;

What is a classification loss function that penalizes overconfidence?;;Log loss, AKA logistic loss or cross-entropy loss.;

Span?;;By scaling your vectors, how many points in a space can you touch? All of them, or less dimensions than that?;

What are norms?;;Ways to measure the size of vectors (length from origin);

Norm formula?;;L1: Σ|x|<br>Ln: Σ(|x|^p)^1/p;

When should you use an L1 over an L2?;;;

Why is it convenient and fast to use the square L2 norm?;;Squared L2 can be computed with X.T @ X<br>Derivatives with respect to each x only require x, whereas L2 norm derivs require whole vector.;

Frobenius norm?;;For taking size of matrices. Analogous to vector L2 norm: you square everything, sum it, then root it.;

Compute cross-entropy/log loss?;;-Σ yᵢ log(ŷᵢ)<br>Sum of the labels times log of predictions.;

How should you imagine an inverse matrix?;;A matrix is a linear transformation into a weird coordinate system: its inverse is playing that backwards (so you're left with the identity: a set of basis vectors!);

How should I imagine matrix multiplication?;;Successive linear transformations.<br>2x2 @ 2x2 results in a 2x2!;

How does Grant Sanderson describe ops like A⁻¹MA?;;As an "empathetic" transformation M (in her language). Start w Jenni's vector _v: A transforms to our language, M is a transform in our language, and A⁻¹ reverts to her language.<img src="translating-a-transform.png">;

How to think about a diagonal matrix?;;All cols are eigenvectors, and those values are eigenvalues!;

How to think of eigenvectors?;;The vectors that stay on their own span during a transformation.;

Singular matrix?;;Square, linearly dependent columns (det = 0). Singular because it's the only kind of matrix without an inverse!;

How should I think about an eigenvector's (_x) and eigenvalue's (x) effect on a matrix?;;X @ _x alters the scale of the eigenvector (by the eigenvalue)<br>Helpful because they tell us how X scales space in each dimension.;

Eigendecomposition?;;Vdiag(λ)V⁻¹<br>V: eigenvectors, λ: diagonal of eigenvalues;

Useful properties of eigendecomposition?;;If any eigenvalues are 0, matrix is singular.<br>Useful for PCA - you can rank eigenvectors by eigenvalues, throw away the rest.;

Linear transformation vs transformation?;;Origin stays put, and gridlines stay parallel and evenly spaced. Other transformations are too complex for linalg!;

Why do we say that matrix columns represent vectors?;;So that matrices represent transforms, and X @ x multiplication represents result of linear transformation on x.<br>Once you start thinking of square matrices as transformations of space, so many concepts get easier: matmul, determinants, eigenv's.;

Positive definite vs positive semidefinite vs negative definite vs negative semidefinite?;;Describes eigenvalues: whether they're all positive/negative and whether they include zeros.;

How do we find the inverse of a matrix that isn't square?;;Moore-Penrose Pseudoinverse, powered by rearranging results of SVD.;

Singular value decomposition?;;UDVᵀ<br>A decomp that exists for all real matrices, helpful for computing inverses.<br>U: left-singular vectors<br>D: singular values across diagonal<br>V: right-singular vectors;

Complex conjugate?;;Complex number, but flip the sign of the imaginary bit. Mirror image/evil twin;

Hermitian matrix and conjugate transpose?;;Like a symmetric matrix for complex numbers.<br>CT: transpose a matrix, take complex conjugates of all values.<br>Hermitian: equal to its own CT.<br>Always have real eigenvalues (same as symmetric m's).;

Cholesky decomposition?;;Only for hermitian, positive definite m's.<br>;

When do matrices have eigendecompositions?;;When they're diagonalizable (similar to a diagonal matrix);

Jacobian?;;For functions whose m inputs and n outputs are vectors: a matrix nxm of partial derivatives.;

Hessian?;;A matrix of second derivatives/a Jacobian of the gradient.;

Condition number?;;If max singular value is big multiple of min singular value (eigenvalues for normal matrices), the matrix is "poorly conditioned". Poorly conditioned Hessians are bad news for gradient descent.;

What can the eigendecomposition of a Hessian tell us?;;The directions and scale of the second derivatives of a function.<br>Positive/negative definite: we're at local minima/maxima.<br>Mixed sign eigenvalues: saddle points.<br>Any zeros: inconclusive.;If our fn has a poorly conditioned Hessian, that tells us that one dimension is changing quickly, and that we should chase in that direction instead of following the steepest first derivative. Imagine standing high on a canyon walls going down a mountain: 1st order with too high a learning rate could zig-zag you across the canyon, 2nd order takes you down the mountain directly.

Newton's method?;;Second-order optimization technique. Uses info from Hessian to guide descent. Can jump right to solution if problem is quadratic. If only approximating quadratic, iteratively use it to jump to solution much faster than grad descent. Attracts to saddle points, unfortunately;

First-order optimization vs second-order optimization?;;Does the algo use just the gradient, or does it also use the Hessian?;

Convex optimization?;;Function for which the Hessian is positive semidefinite everywhere. Great for second-order optimization.;

MLE?;;Maximum likelihood estimation.<br>Estimate the distribution given a sample (i.e., estimate μ, σ² for all dims)<br>;

Calculus: product rule? fg;;;f'g + fg'

Calculus: quotient rule? f/g;;;(f'g - fg')/g²

Calculus: reciprocal rule? 1/f;;;-f'/f²

GBMs vs RFs?;;GBMs: start with high bias, low variance tree. Reduce by training additional trees on residuals.<br>RFs: low bias, high variance. Reduce variance by ensembling.;

Why are sparse matrices good for performance?;;Can skip multiply/add by zero<br>Way smaller;

Count values in a Series that satisfy some predicate?;;;sum(train_y > 2)

Count rows in a DataFrame that have any value that satisfies some predicate?;;;(train_X == 0).any(axis=0)

Describe the Word2Vec skip gram architecture;;Train simple NN with 1 hidden layer to predict the middle word in a phrase.;

How is an autoencoders useful? They just pass inputs through a hidden layer and output the same thing, right?;;After training, you can use the hidden layer - it'll have learned useful features.;

Autoencoder vs variational autoencoder?;;VAE: loss function includes KL divergence between input encoding and Gaussian uniform vector;

Stochastic gradient descent vs gradient descent?;;Gradient descent: compute gradient for entire set.<br>Stochastic: approximate that gradient by selecting a random batch.;

Cosine distance vs Euclidean distance?;;Cosine = angle between vectors, Euclidean = distance. For high dim space (100s+), distance is meaningless because everything is "on the edge" of that space.;

Find the most similar vector?;;;sklearn.neighbors.NearestNeighbors

sklearn.neighbors: NearestNeighbors vs LSHForest?;;Exact NN (n^2) vs approximate NN (nlog(n)) - locality sensitive hashing forests.;

Convert bcolz array into numpy array?;;Careful you have enough memory tho.;bcolz.open('thing.bc')[:]

Style target for style transfer?;;Gram matrix of image channels. Somehow removes location info, preserves texture info;

Dropout?;;Regularization technique. Analogous to training a large ensemble of small networks.;

Why do word embeddings work by counting context words?;;Because the meaning of "club" in "a ___ walked into a club" depends on if ___ is "DJ" or "baby seal".;

Bilinear interpolation?;;For a point within a field, take the linear average in two dimensions. Compare to nearest-neighbor interpolation.<img src="bilinear-interpolation.png">;

Describe this convolution?<img src="conv-stride.png">;;3x3 filter, stride 2;

How are resnets like ensembles?;;It's in the title: "Residual networks are exponential ensembles of relatively shallow networks", Andreas Veit, May 16;

Momentum vs Nesterov momentum?;;Evaluate gradient at current position, or evalute gradient at end of momentum step? Nesterov consistently a bit better.;

L-BFGS?;;Approximates Newton's method by watching gradients over time to form an approximate Hessian.;

Adagrad, RMSProp?;;Optimization algorithms that maintain different LRs for each param based on magnitudes of their gradients.;

Adam? Kingma, Q4 2014;;RMSProp with momentum.<br>Often worth trying SGD + Nesterov momentum too, though.;

Why are autoencoders a Bad Idea for unsupervised learning?;;Because they learn to predict the present.<br>Real unsupervised learning happens when you use causal prediction as the supervisory signal (predict next word, next frame) (LSTMs)<br>See: language modeling, autoregressive image generators (PixelRNN) that produce sharper images than non-autoregressive VAEs;

Attention Is All You Need?;;seq2seq. RNNs/CNNs are unparallelizable. Transformer iteratively generates embeddings with multi-headed attention. 2x faster to train, beat SOTA by 2 BLEU;Jun 17, Ashish Vaswani

Artistic style transfer for videos?;;style transfer each frame + a penalty term for diverging from the previous frame + tricks to make it look good:<br>1. penalize deviations along "optical flow"<br>2. don't penalize disoccluded spots<br>3. don't let artifacts at the frame edges form;Apr '16

How does a Pixel RNN (Jan '16) create a decent distribution of images?;;By using 2D RNNs to scan across pixels, and adding RNN skip connections.;

How did Lecun smooth high-variance per-pixel predictions in Learning Hierarchical Features for Scene Labeling? (2013);;Well first he predicted a class for every pixel at 3 different levels of granularity (3-level Laplacian pyramid)<br>Then he smoothed noisy predictions by identifying hierarchies of superpixel regions of similar color intensities. Finally he chose labels and regions that minimize surprisingness w a tree structure.<br>;

Laplacian pyramid?;;Multiple scaled copies of an image. Each image is a difference image of one below, lowest-res is pixels. Useful for compression, image segmentation.<br>Compare to Gaussian pyramids: each level is blurred and downsampled.;

Superpixels?;;Image segmentation<br>Hierarchical regions of similar color intensity. Follow natural contours of image.;

How do Recursive Context Propagation Networks resolve confusing image segmentation problems, like whether something is a "boat" or a "building"?;;They used an RNN to add context from other class probabilities, e.g. by noticing that the nearby segment is labeled "water".;

How might you tune your performance metric if one type of mistake is more costly than another?;;Make the cost of certain mistakes higher than others.;

How do you design a performance metric for a rare disease detection system with 99.999% negative examples?;;Instead of measuring error, measure precision (% of predictions that are correct) and recall (% of true events predicted).<br><img src="precision-recall.png">;

If you're considering letting your system not make predictions if its below a certain confidence, what should you measure?;;Coverage - % of things it made a prediction for<br>Accuracy - % of good predictions. OR<br>Precision & recall.;

If your training performance is bad, what should you try?;;Increase model's capacity<br>Tune your LR<br>Improve the quality of your data;

If your test performance is bad, what should you try?;;Reduce the model size<br>Regularize<br>Estimate how much data you need.;

If training error is low and test error is high, how can you brute force your model?;;Increase capacity and add data.;

WGAN?;;Wasserstein GAN. Original GANs used JS divergence for loss, but this isn't defined everywhere.<br>WGANs use Earth Mover distance: continuous, but intractable. So approximate it with another NN.<br>Jan 2017.;

DCGAN?;;Deep conv GANs - generate realistic images.;

DiscoGAN/CycleGAN?;;Cross-domain relations - handbags to shoes, horses to zebras. Different loss functions.;

Dynamic Routing Between Capsules?;Hinton, Q4 '17;A capsule = a groups of conv filters that output a vector, compared to CNNs which output one value for feature detector. Capsule length represents likelihood of that part existing in input.<br>Margin loss: length of final capsules compared to one-hot encoded input, reconstructive loss: decode vector into image and compare to input (encourages capsules to encode meaningful features)<br>SOTA MNIST, 10.2% on CIFAR10<br>Uses attention-like "routing by agreement" idea to route low-level capsules to high-level capsules whose activations point in similar direction. Replaces pooling, where you lose tons of info;

Polygon RNN?;;Predict polygons from clicks, 5-7x speedup w 80% agreement;

How did SynthMed generate such a realistic retinal image dataset?;;They used real segmented retinal images and generated the contents;

Synthetic EHRs: name, pros and cons?;;medGAN. Generally indistinguishable except for outliers (prostate cancer (male) + menopausal disorder in same record - could be weeded out with domain-specific rules). Great for avoiding privacy concerns.;

Training object class detectors with click supervision?;;Weakly supervised object localization: you click middle, it draws bbox. Big speedup;

Cut/Paste/Learn: Synthesis for Instance Detection;;Put real object instances on fake bgs, train. 90% synthetic data outperforms models trained on 100% real data!;

Generate 100% synthetic medical time series data? Pros, performance comparison?;;RGAN (Recurrent GAN), generates time series data w labels (trained early warning system on EHRs from ICUs). Models could train on 100% synthetic data, still generalize well (kinda like ImageNet), no privacy concerns, almost the same performance.;

Knowledge distillation?;;Use an ensemble of models' predictions to train a single model - faster inference, obfuscates training data used.;

Active learning?;;Decide which images to label next;

Online learning?;;Continually train model with new examples;

Weak supervision?;;Use clues.;

Why is it cool to generate synthetic segmentation masks and then convert them to images?;;In healthcare settings, training on this synthetic data can sidestep privacy concerns. Performance is on par: 1% difference in F1 score. Dual Generative Adversarial Networks.;

U-Net?;;Biomedical image segmentation that's fast and requires <30 examples. U-shaped architecture max-pools then deconvs. Use lots of biomedical-specific data aug (warping, fuzzy membranes). Foreground/background class.;
