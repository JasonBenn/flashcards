How can you recognize overfitting?;;Training MSE is small, testing MSE is large.<br>Reduce the flexibility of your model.;

Softmax?;;Function that provides a probability for each of N classes, total is 1.;

When is the span of two vectors *not* any point on their plane?;;When aligned: you'd only be able to reach points on a single line. Also, when they're both zero.;

Linear dependence?;;When you can remove at least one vector without affecting the span.;

If a matrix's determinant is zero, what does that mean?;;The transformation collapsed along at least one dimension, so the volume is now 0. Columns (transf. vectors) must be linearly dependent.;

Determinant?;;How much areas change due to the transformation defined by the matrix's vectors (so it has to be square). Imagine a parallelepiped getting stretched out or compressed (or inverted).;

Dot product?;;Project one vector onto another, multiply lengths. Dot product of 0 = vectors are orthogonal. Helpful to understand a vector as a transformation.;

np: vdot vs dot?;;dot transforms vectors in matrices, vdot flattens matrices into vectors.;

np: Euclidean norm of a matrix?;;AKA the Frobenius norm<br>ord: order of the norm or 'fro'<br>axis: int or 2-tuple of int<br>keepdims=False: keep axes that were normed so broadcast works (?);np.linalg.norm(x, 'fro')

Matrix rank?;;Dimensionality of vector space spanned by matrix's column vectors AKA # of linearly independent vectors of matrix.;

np: sum diagonal of matrix?;;;np.trace(x)

Inverse of a matrix?;;When you multiply it by your matrix, it results in an identity matrix.;x @ y == np.identity(n)

Perplexity?;;How well a prob distribution predicts a sample. Low perplexity means is good/unsurprising.;

Log loss?;;Classification loss function that heavily penalizes overconfidence;

Span?;;By scaling your vectors, how many points in a space can you touch? All of them, or less dimensions than that?;
